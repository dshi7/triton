import pathlib
import re

import pytest
import torch
import triton

from triton._internal_testing import is_cuda


# Generated from python/tutorials/01-vector-add.py
def get_ptx_src():
    return """
//
// Generated by LLVM NVPTX Back-End
//

.version 8.4
.target sm_90a
.address_size 64

	// .globl	add_kernel              // -- Begin function add_kernel
                                        // @add_kernel
.visible .entry add_kernel(
	.param .u64 add_kernel_param_0,
	.param .u64 add_kernel_param_1,
	.param .u64 add_kernel_param_2,
	.param .u32 add_kernel_param_3
)
.reqntid 128, 1, 1
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<7>;
	.reg .b32 	%r<7>;
	.reg .b64 	%rd<8>;
	.loc	1 15 0
$L__func_begin0:
	.loc	1 15 0

// %bb.0:
	ld.param.u64 	%rd4, [add_kernel_param_0];
	ld.param.u64 	%rd5, [add_kernel_param_1];
$L__tmp0:
	.loc	1 25 24
	// begin inline asm
	mov.u32 %r1, %ctaid.x;
	// end inline asm
	.loc	1 30 24
	shl.b32 	%r2, %r1, 7;
	ld.param.u64 	%rd6, [add_kernel_param_2];
	ld.param.u32 	%r3, [add_kernel_param_3];
	.loc	1 31 41
	mov.u32 	%r4, %tid.x;
	and.b32  	%r5, %r4, 127;
	.loc	1 31 28
	or.b32  	%r6, %r2, %r5;
	.loc	1 33 21
	setp.lt.s32 	%p1, %r6, %r3;
	.loc	1 36 24
	mul.wide.s32 	%rd7, %r6, 2;
	add.s64 	%rd1, %rd4, %rd7;
	.loc	1 36 16
	// begin inline asm
	mov.u16 %rs4, 0x0;
	@%p1 ld.global.b16 { %rs4 }, [ %rd1 + 0 ];
	// end inline asm
	.loc	1 37 24
	add.s64 	%rd2, %rd5, %rd7;
	.loc	1 37 16
	// begin inline asm
	mov.u16 %rs5, 0x0;
	@%p1 ld.global.b16 { %rs5 }, [ %rd2 + 0 ];
	// end inline asm
	.loc	1 38 17
	// begin inline asm
	{ .reg .b16 c;
   mov.b16 c, 0x3f80U;
   fma.rn.bf16 %rs6, %rs4, c, %rs5; }

	// end inline asm
	.loc	1 40 26
	add.s64 	%rd3, %rd6, %rd7;
	.loc	1 40 35
	// begin inline asm
	@%p1 st.global.b16 [ %rd3 + 0 ], { %rs6 };
	// end inline asm
	.loc	1 40 4
	ret;
$L__tmp1:
$L__func_end0:
                                        // -- End function
}
    """


def get_ttir_src():
    return """
module {
  tt.func public @add_kernel(%arg0: !tt.ptr<bf16>, %arg1: !tt.ptr<bf16>, %arg2: !tt.ptr<bf16>, %arg3: i32) attributes {noinline = false} {
    %c128_i32 = arith.constant 128 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.muli %0, %c128_i32 : i32
    %2 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>
    %3 = tt.splat %1 : i32 -> tensor<128xi32>
    %4 = arith.addi %3, %2 : tensor<128xi32>
    %5 = tt.splat %arg3 : i32 -> tensor<128xi32>
    %6 = arith.cmpi slt, %4, %5 : tensor<128xi32>
    %7 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<128x!tt.ptr<bf16>>
    %8 = tt.addptr %7, %4 : tensor<128x!tt.ptr<bf16>>, tensor<128xi32>
    %9 = tt.load %8, %6 : tensor<128x!tt.ptr<bf16>>
    %10 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<128x!tt.ptr<bf16>>
    %11 = tt.addptr %10, %4 : tensor<128x!tt.ptr<bf16>>, tensor<128xi32>
    %12 = tt.load %11, %6 : tensor<128x!tt.ptr<bf16>>
    %13 = arith.addf %9, %12 : tensor<128xbf16>
    %14 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<128x!tt.ptr<bf16>>
    %15 = tt.addptr %14, %4 : tensor<128x!tt.ptr<bf16>>, tensor<128xi32>
    tt.store %15, %13, %6 : tensor<128x!tt.ptr<bf16>>
    tt.return
  }
}
"""


@pytest.mark.skipif(not is_cuda(), reason="PTX is only supported on NVIDIA")
def test_ptx_add_vec_simple(tmp_path: pathlib.Path):
    src = get_ptx_src()
    temp_file = tmp_path / "test_add_vec_kernel.ptx"
    temp_file.write_text(src)
    # src = get_ttir_src()
    # temp_file = tmp_path / "test_add_vec_kernel.ttir"
    # temp_file.write_text(src)

    kernel = triton.compile(
        src=str(temp_file), target=triton.runtime.driver.active.get_current_target()
    )

    torch.manual_seed(0)
    size = 20
    DEVICE = torch.device("cuda")
    x = torch.rand(size, device=DEVICE, dtype=torch.bfloat16)
    y = torch.rand(size, device=DEVICE, dtype=torch.bfloat16)
    output = torch.empty_like(x)

    kernel[(1, 1, 1)](
        x.data_ptr(),
        y.data_ptr(),
        output.data_ptr(),
        # x,
        # y,
        # output,
        size,
    )

    assert (
        kernel.metadata.shared
        == triton.runtime.driver.active.utils.get_device_properties(
            triton.runtime.driver.active.get_current_device()
        )["max_shared_mem"]
    )

    assert torch.allclose(output, x + y, atol=1e-3)


@pytest.mark.skipif(not is_cuda(), reason="PTX is only supported on NVIDIA")
def test_ptx_add_vec_user_defined_smem(tmp_path: pathlib.Path):

    src = get_ptx_src()
    # User can set smem size in PTX comment (in bytes)
    src = "// user_defined_smem_size 128000 \n" + src

    temp_file = tmp_path / "test_add_vec_kernel.ptx"
    temp_file.write_text(src)

    kernel = triton.compile(
        src=str(temp_file), target=triton.runtime.driver.active.get_current_target()
    )

    torch.manual_seed(0)
    size = 20
    DEVICE = torch.device("cuda")
    x = torch.rand(size, device=DEVICE, dtype=torch.bfloat16)
    y = torch.rand(size, device=DEVICE, dtype=torch.bfloat16)
    output = torch.empty_like(x)

    kernel[(1, 1, 1)](
        x.data_ptr(),
        y.data_ptr(),
        output.data_ptr(),
        size,
    )
    assert (
        kernel.metadata.shared == 128000
    ), "user defined smem size is not passed to compiler"
    assert torch.allclose(output, x + y, atol=1e-3)


@pytest.mark.skipif(not is_cuda(), reason="PTX is only supported on NVIDIA")
def test_ptx_mul_vec(tmp_path: pathlib.Path):

    src = get_ptx_src()
    # Manually modify PTX source code for multiplication
    src = re.sub("mov.b16 c, 0x3f80U;", "mov.b16 c, 0x8000U;", src, flags=re.MULTILINE)
    src = re.sub(
        "fma.rn.bf16 %rs6, %rs4, c, %rs5;",
        "fma.rn.bf16 %rs6, %rs4, %rs5, c;",
        src,
        flags=re.MULTILINE,
    )

    temp_file = tmp_path / "test_mul_vec_kernel.ptx"
    temp_file.write_text(src)

    kernel = triton.compile(
        src=str(temp_file), target=triton.runtime.driver.active.get_current_target()
    )

    torch.manual_seed(0)
    size = 20
    DEVICE = torch.device("cuda")
    x = torch.rand(size, device=DEVICE, dtype=torch.bfloat16)
    y = torch.rand(size, device=DEVICE, dtype=torch.bfloat16)
    output = torch.empty_like(x)

    kernel[(1, 1, 1)](
        x.data_ptr(),
        y.data_ptr(),
        output.data_ptr(),
        size,
    )

    assert torch.allclose(output, x * y, atol=1e-3)
